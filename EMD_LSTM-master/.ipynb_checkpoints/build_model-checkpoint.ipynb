{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from utils.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from numpy import newaxis\n",
    "from keras.layers import Dense, Activation, Dropout, LSTM\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.callbacks import TensorBoard\n",
    "from utils import Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    \"\"\"A class for an building and inferencing an lstm model\"\"\"\n",
    "    def __init__(self):\n",
    "        self.model = Sequential()\n",
    "    def load_model(self, filepath):\n",
    "        print('[Model] Loading model from file %s' % filepath)\n",
    "        self.model = load_model(filepath)\n",
    "    def build_model(self, configs):\n",
    "        timer = Timer()\n",
    "        timer.start()\n",
    "        for layer in configs['model']['layers']:\n",
    "            neurons = layer['neurons'] if 'neurons' in layer else None\n",
    "            dropout_rate = layer['rate'] if 'rate' in layer else None\n",
    "            activation = layer['activation'] if 'activation' in layer else None\n",
    "            return_seq = layer['return_seq'] if 'return_seq' in layer else None\n",
    "            input_timesteps = layer['input_timesteps'] if 'input_timesteps' in layer else None\n",
    "            input_dim = layer['input_dim'] if 'input_dim' in layer else None\n",
    "            if layer['type'] == 'dense':\n",
    "                self.model.add(Dense(neurons, activation=activation))\n",
    "            if layer['type'] == 'lstm':\n",
    "                self.model.add(LSTM(neurons, input_shape=(input_timesteps, input_dim),return_sequences=return_seq))\n",
    "            if layer['type'] == 'dropout':\n",
    "                self.model.add(Dropout(dropout_rate))\n",
    "        self.model.compile(loss=configs['model']['loss'], optimizer=configs['model']['optimizer'])\n",
    "        print('[Model] Model Compiled')\n",
    "        timer.stop()\n",
    "    \n",
    "    def model_to_json(self,save_dir):\n",
    "        model_json = self.model.to_json()\n",
    "        fname = os.path.join(save_dir, 'model.json')\n",
    "        with open(fname, \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "        print('[Model] Serialize model to JSON at %s' % fname)\n",
    "            \n",
    "    def train(self, x, y, epochs, batch_size, save_dir):\n",
    "        timer = Timer()\n",
    "        timer.start()\n",
    "        print('[Model] Training Started')\n",
    "        print('[Model] %s epochs, %s batch size' % (epochs, batch_size))\n",
    "        \n",
    "        save_fname = os.path.join(save_dir, '%s-e%s.h5' % (dt.datetime.now().strftime('%d%m%Y-%H%M%S'), str(epochs)))\n",
    "        callbacks = [\n",
    "            EarlyStopping(monitor='val_loss', patience=2),\n",
    "            ModelCheckpoint(filepath=save_fname, monitor='loss', save_best_only=True)\n",
    "        ]\n",
    "        self.model.fit(\n",
    "            x,\n",
    "            y,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "        self.model.save(save_fname)\n",
    "        print('[Model] Training Completed. Model saved as %s' % save_fname)\n",
    "        timer.stop()\n",
    "        \n",
    "    def train_generator(self,data_gen,val_gen,epochs, batch_size,steps_per_epoch,log_fname,save_fname):\n",
    "        timer = Timer()\n",
    "        timer.start()\n",
    "        print('[Model] Training Started')\n",
    "        print('[Model] %s epochs, %s batch size, %s batches per epoch' % (epochs, batch_size,steps_per_epoch))\n",
    "\n",
    "        callbacks = [\n",
    "            ModelCheckpoint(filepath=save_fname, monitor='val_loss', save_best_only=True),\n",
    "            TensorBoard(log_dir=log_fname, histogram_freq=0,write_graph=True, write_images=True)\n",
    "        ]\n",
    "        self.model.fit_generator(\n",
    "            data_gen,\n",
    "            validation_data=val_gen,\n",
    "            validation_steps=1,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            epochs=epochs,\n",
    "            callbacks=callbacks,\n",
    "#             use_multiprocessing = True,\n",
    "            workers=1\n",
    "        )\n",
    "        \n",
    "        print('[Model] Training Completed. Model saved as %s' % save_fname)\n",
    "        timer.stop()\n",
    "        \n",
    "    def predict_point_by_point(self, data):\n",
    "        #Predict each timestep given the last sequence of true data, in effect only predicting 1 step ahead each time\n",
    "        print('[Model] Predicting Point-by-Point...')\n",
    "        predicted = self.model.predict(data)\n",
    "#         print('before the predicted size is', predicted.shape )\n",
    "        predicted = np.reshape(predicted, (predicted.size,))\n",
    "        return predicted\n",
    "    \n",
    "    def evaluate(self, x_test, y_test):\n",
    "        score = self.model.evaluate(x=x_test, y=y_test)\n",
    "        return score\n",
    "    \n",
    "    def predict_sequences_multiple(self, data, window_size, prediction_len):\n",
    "        #Predict sequence of 50 steps before shifting prediction run forward by 50 steps\n",
    "        print('[Model] Predicting Sequences Multiple...')\n",
    "        prediction_seqs = []\n",
    "#         print('length of test data',len(data), 'prediction length:', prediction_len)\n",
    "        for i in range(int(len(data)/prediction_len)):\n",
    "            curr_frame = data[i*prediction_len]\n",
    "            \n",
    "            predicted = []\n",
    "            for j in range(prediction_len):\n",
    "#                 print(self.model.predict(curr_frame[newaxis,:,:]))\n",
    "#                 print(self.model.predict(curr_frame[newaxis,:,:])[0,0])\n",
    "                predicted.append(self.model.predict(curr_frame[newaxis,:,:])[0,0])\n",
    "                \n",
    "                curr_frame = curr_frame[1:]\n",
    "                curr_frame = np.insert(curr_frame, [window_size-2], predicted[-1], axis=0)\n",
    "            prediction_seqs.append(predicted)\n",
    "        return prediction_seqs\n",
    "    \n",
    "    def predict_sequence_full(self, data, window_size):\n",
    "        #Shift the window by 1 new prediction each time, re-run predictions on new window\n",
    "        print('[Model] Predicting Sequences Full...')\n",
    "        curr_frame = data[0]\n",
    "        predicted = []\n",
    "        for i in range(len(data)):\n",
    "            predicted.append(self.model.predict(curr_frame[newaxis,:,:])[0,0])\n",
    "            curr_frame = curr_frame[1:]\n",
    "            curr_frame = np.insert(curr_frame, [window_size-2], predicted[-1], axis=0)\n",
    "        return predicted"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
